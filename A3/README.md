
### Instructions to Run: 
- To Train the model: `python3 A3/main.py data/` 
- To Test the model: `python3 A3/inference.py data/sample.txt`

### Most similar words: Good

- --------------- good : Most similar words---------------
great --> 0.7400978207588196
decent --> 0.7308747172355652
fantastic --> 0.6591929793357849
nice --> 0.6526317596435547
terrific --> 0.632607638835907
wonderful --> 0.6227951645851135
superb --> 0.6158816814422607
fabulous --> 0.601729691028595
bad --> 0.5932040214538574
poor --> 0.5905114412307739
terrible --> 0.576059877872467
impressive --> 0.5750510692596436
reasonable --> 0.5740842819213867
excellent --> 0.57100909948349
horrible --> 0.5590829253196716
pleasant --> 0.5544540882110596
awesome --> 0.5368488430976868
neat --> 0.5273263454437256
lousy --> 0.5204532742500305
okay --> 0.5198305249214172


--------------- bad : Most similar words---------------
horrible --> 0.6502007246017456
terrible --> 0.6196836233139038
good --> 0.5932040214538574
awful --> 0.5671632885932922
lame --> 0.5395219922065735
poor --> 0.5368082523345947
funny --> 0.5327811241149902
lousy --> 0.512424886226654
nasty --> 0.5047847032546997
strange --> 0.4981611967086792
obvious --> 0.49523860216140747
fake --> 0.49145179986953735
stupid --> 0.490740031003952
weak --> 0.48993155360221863
faint --> 0.48430517315864563
gross --> 0.48382842540740967
weird --> 0.4757237732410431
loud --> 0.47547003626823425
overpowering --> 0.4689329266548157
harsh --> 0.46393170952796936

### Are the words most similar to “good” positive, and words most similar to “bad” negative?
- No, Not all the words most similar to good positive
- No, Not all the words most similar to bad negative 
- Note: Tried using multiple hyperparameters, similar patterns were seen.

### Analysis: 
1. The most similar words are generated by finding the cosine distance(dot product of vectors) which is ∝ similarity between the words. Words like "good" and "bad" are very far apart semantically but are appearing in the context:
    - Ex: 
        1. It looks nice, but doesn't work like the *good* and oldie!Too *bad*.
        2. But there were more *bad* pots than *good*.
        3. In choosing the Waring model, I researched and read many Amazon reviews, both *good* and *bad*.
        4. The *good*: the non-stick surface works great and is easy to clean.The *bad*: does a terrible job cooking rice.
    - Window size plays an important role while the training phase as the network learns the statistics from the number of times each pairing shows up. In other words, the vector of two words will be pulled closer.
2. The negation can also be one of the reason for the same.

#### References:
- https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/
- http://jalammar.github.io/illustrated-word2vec/
- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
- https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485