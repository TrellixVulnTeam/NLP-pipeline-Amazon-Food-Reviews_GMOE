
### Instructions to Run: 
- To Train the model: `python3 A3/main.py data/` 
- To Test the model: `python3 A3/inference.py data/sample.txt`

### Most similar words: Good

- great --> 0.7551022171974182
- decent --> 0.7508329749107361
- fantastic --> 0.6584885120391846
- nice --> 0.6501882076263428
- wonderful --> 0.6439023017883301
- bad --> 0.6197049617767334
- superb --> 0.6087694764137268
- excellent --> 0.6060928702354431
- terrible --> 0.5937264561653137
- terrific --> 0.5934835076332092
- reasonable --> 0.5878351926803589
- poor --> 0.5779930353164673
- impressive --> 0.5588662624359131
- fair --> 0.5476508140563965
- horrible --> 0.5454399585723877
- okay --> 0.5450385808944702
- awesome --> 0.5421015620231628
- pleasant --> 0.5388332605361938
- strong --> 0.5357953310012817
- amazing --> 0.5348005890846252


### Most similar words: Bad

- horrible --> 0.6506983041763306
- terrible --> 0.6456992626190186
- good --> 0.6197049617767334
- awful --> 0.5941882133483887
- lame --> 0.5502369999885559
- Faced --> 0.5411893129348755
- weak --> 0.5405016541481018
- funny --> 0.5383061766624451
- nasty --> 0.5296658873558044
- poor --> 0.5269413590431213
- obvious --> 0.509117841720581
- fake --> 0.50090092420578
- stupid --> 0.4993632137775421
- strong --> 0.49160268902778625
- lousy --> 0.49101123213768005
- strange --> 0.48473429679870605
- loud --> 0.4782383441925049
- upset --> 0.47385603189468384
- weird --> 0.473296195268631
- overpowering --> 0.46788638830184937

### Are the words most similar to “good” positive, and words most similar to “bad” negative?
- *"Good"*: No, Not all but most of the words most similar to good are positive. Exception: "bad" "terrible" 
- *"Bad"*: No, Not all but most of the words most similar to bad are negative. Exception: "good"
- _Note_: Tried using multiple hyperparameters, similar patterns were seen.

### Analysis: 
1. The most similar words are generated by finding the cosine distance(dot product of vectors) which is ∝ similarity between the words. Word2vec's objective function is trying to maximize the magnitude of dot product of wv and its context wv which determines the similarity. Words like "good" and "bad" are very far apart semantically but are appearing in the same context.
    - Ex: 
        1. It looks nice, but doesn't work like the *good* and oldie!Too *bad*.
        2. But there were more *bad* pots than *good*.
        3. In choosing the Waring model, I researched and read many Amazon reviews, both *good* and *bad*.
        4. The *good*: the non-stick surface works great and is easy to clean.The *bad*: does a terrible job cooking rice.
    - Window size plays an important role while the training phase as the network learns the statistics from the number of times each pairing shows up. In other words, the vector of two words will be pulled closer.
    - So, the similarity is more if the words are closer in the corpus very often which is happening in our case for *good* and *bad* as seen in above examples.
2. The negation or interchangeable usage in the same context can also be one of the reason for the same. Ex: {"good","bad"}, {"cheap","expensive"} used interchangeably.


#### References:
- https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/
- http://jalammar.github.io/illustrated-word2vec/
- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
- https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485