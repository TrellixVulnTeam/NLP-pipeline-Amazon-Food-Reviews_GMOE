
### Instructions to Run: 
- To Train the model: `python3 A3/main.py data/` 
- To Test the model: `python3 A3/inference.py data/sample.txt`

# Most similar words: Good

- great --> 0.7460801005363464
- decent --> 0.7353028655052185
- nice --> 0.6631649732589722
- fantastic --> 0.6491502523422241
- wonderful --> 0.6353268623352051
- terrific --> 0.6318401098251343
- superb --> 0.6142178773880005
- bad --> 0.6067554950714111
- poor --> 0.6029418110847473
- reasonable --> 0.588286280632019
- excellent --> 0.5877429246902466
- terrible --> 0.58246248960495
- fabulous --> 0.5750617980957031
- impressive --> 0.5748500227928162
- horrible --> 0.5530016422271729
- strong --> 0.550117552280426
- pleasant --> 0.5459936857223511
- lovely --> 0.5380986928939819
- neat --> 0.5320737957954407
- fair --> 0.5258995890617371


--------------- bad : Most similar words---------------
- terrible --> 0.6374627947807312
- horrible --> 0.6151857376098633
- good --> 0.6067554950714111
- awful --> 0.5769482851028442
- poor --> 0.5422894954681396
- lame --> 0.5354527235031128
- stupid --> 0.5126662254333496
- funny --> 0.5093436241149902
- weak --> 0.5075661540031433
- strong --> 0.5037192702293396
- lousy --> 0.5007032155990601
- nasty --> 0.49756693840026855
- fake --> 0.4962286055088043
- weird --> 0.49606063961982727
- strange --> 0.49428483843803406
- obvious --> 0.4868602156639099
- dumb --> 0.481082558631897
- loud --> 0.4769476056098938
- sad --> 0.4593732953071594
- dud --> 0.4580140709877014


# Are the words most similar to “good” positive, and words most similar to “bad” negative?
- No, Not all the words most similar to good positive
- No, Not all the words most similar to bad negative 
- Note: Tried using multiple hyperparameters, similar patterns were seen.

# Reason: 
1. The most similar words are generated by finding the cosine distance(dot product of vectors) which is ∝ similarity between the words. Words like "good" and "bad" are very far apart semantically but are appearing in the context:
    - Ex: 
        1. It looks nice, but doesn't work like the good and oldie!Too bad.
        2. But there were more bad pots than good.
        3. In choosing the Waring model, I researched and read many Amazon reviews, both good and bad.
        4. The good: the non-stick surface works great and is easy to clean.The bad: does a terrible job cooking rice.
    - Window size plays an important role while the training phase as the network learns the statistics from the number of times each pairing shows up. In other words, the vector of two words will be pulled closer
2. 

https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/
http://jalammar.github.io/illustrated-word2vec/
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L485